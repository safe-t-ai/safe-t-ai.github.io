name: Data Pipeline

on:
  schedule:
    # Run weekly on Mondays at 6 AM UTC (census data doesn't change often)
    - cron: '0 6 * * 1'
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - 'scripts/**'
      - '.github/workflows/data-pipeline.yml'
  workflow_dispatch:
    inputs:
      force_regenerate:
        description: 'Force regenerate all data'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.9'

jobs:
  fetch-census-data:
    runs-on: ubuntu-latest
    outputs:
      data-hash: ${{ steps.compute-hash.outputs.hash }}
      should-skip: ${{ steps.check-changed.outputs.skip }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Restore previous data hash
        if: github.event.inputs.force_regenerate != 'true'
        id: cache-hash
        uses: actions/cache/restore@v4
        with:
          path: .data-hash
          key: durham-data-hash-${{ github.run_id }}
          restore-keys: durham-data-hash-

      - name: Fetch Durham census data
        env:
          CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}
        run: |
          cd backend
          python ../scripts/fetch_durham_data.py

      - name: Compute data hash
        id: compute-hash
        run: |
          HASH=$(sha256sum backend/data/raw/durham_census_tracts.geojson | cut -c1-16)
          echo "$HASH" > .data-hash
          echo "hash=$HASH" >> $GITHUB_OUTPUT

          if [ -f ".data-hash.prev" ] && [ "${{ github.event.inputs.force_regenerate }}" != "true" ]; then
            PREV_HASH=$(cat .data-hash.prev)
            echo "previous-hash=$PREV_HASH" >> $GITHUB_OUTPUT
          else
            echo "previous-hash=" >> $GITHUB_OUTPUT
          fi

      - name: Check if data changed
        id: check-changed
        run: |
          CURRENT="${{ steps.compute-hash.outputs.hash }}"
          PREVIOUS="${{ steps.compute-hash.outputs.previous-hash }}"
          FORCE="${{ github.event.inputs.force_regenerate }}"

          if [[ "${{ github.event_name }}" == "schedule" && "$CURRENT" == "$PREVIOUS" && -n "$PREVIOUS" && "$FORCE" != "true" ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "ðŸ“Š Data unchanged: $CURRENT (scheduled run, skipping pipeline)"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
            if [[ "${{ github.event_name }}" == "schedule" ]]; then
              echo "ðŸ“Š Data changed: $PREVIOUS -> $CURRENT"
            elif [[ "$FORCE" == "true" ]]; then
              echo "ðŸ“Š Force regenerate requested"
            else
              echo "ðŸ“Š Running pipeline (triggered by: ${{ github.event_name }})"
            fi
          fi

      - name: Save data hash
        if: steps.check-changed.outputs.skip != 'true'
        uses: actions/cache/save@v4
        with:
          path: .data-hash
          key: durham-data-hash-${{ github.run_id }}

      - name: Generate job summary
        run: |
          echo "## ðŸ“Š Durham Census Data" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f backend/data/raw/durham_census_tracts.geojson ]; then
            TRACTS=$(python3 -c "import json; d=json.load(open('backend/data/raw/durham_census_tracts.geojson')); print(len(d['features']))" 2>/dev/null || echo "N/A")
            SIZE=$(du -h backend/data/raw/durham_census_tracts.geojson | cut -f1)

            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Census Tracts | $TRACTS |" >> $GITHUB_STEP_SUMMARY
            echo "| File Size | $SIZE |" >> $GITHUB_STEP_SUMMARY
            echo "| Data Hash | ${{ steps.compute-hash.outputs.hash }} |" >> $GITHUB_STEP_SUMMARY

            if [ "${{ steps.check-changed.outputs.skip }}" == "true" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "> â­ï¸ Skipping remaining pipeline: data unchanged" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Upload census data artifact
        if: steps.check-changed.outputs.skip != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: census-data
          path: backend/data/raw/durham_census_tracts.geojson
          retention-days: 7

  simulate-ai-predictions:
    needs: fetch-census-data
    if: needs.fetch-census-data.outputs.should-skip != 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Download census data
        uses: actions/download-artifact@v4
        with:
          name: census-data
          path: backend/data/raw

      - name: Simulate AI predictions with bias
        run: |
          cd backend
          python ../scripts/simulate_ai_predictions.py

      - name: Simulate infrastructure recommendations
        run: |
          cd backend
          python ../scripts/simulate_infrastructure_recommendations.py

      - name: Simulate crash predictions
        run: |
          cd backend
          python ../scripts/simulate_crash_predictions.py

      - name: Analyze suppressed demand
        run: |
          cd backend
          python ../scripts/analyze_suppressed_demand.py

      - name: Generate job summary
        run: |
          echo "## ðŸ¤– AI Predictions Simulation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f backend/data/simulated/ai_volume_predictions.json ]; then
            PREDICTIONS=$(python3 -c "import json; d=json.load(open('backend/data/simulated/ai_volume_predictions.json')); print(len(d))" 2>/dev/null || echo "N/A")
            AVG_ERROR=$(python3 -c "import json; d=json.load(open('backend/data/simulated/ai_volume_predictions.json')); errors=[x['error_pct'] for x in d]; print(f'{sum(errors)/len(errors):.1f}%')" 2>/dev/null || echo "N/A")

            echo "### Test 1: Volume Predictions" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Counter Locations | $PREDICTIONS |" >> $GITHUB_STEP_SUMMARY
            echo "| Average Bias | $AVG_ERROR |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f backend/data/simulated/infrastructure_recommendations.json ]; then
            AI_PROJECTS=$(python3 -c "import json; d=json.load(open('backend/data/simulated/infrastructure_recommendations.json')); print(d['summary']['ai_projects'])" 2>/dev/null || echo "N/A")
            NEED_PROJECTS=$(python3 -c "import json; d=json.load(open('backend/data/simulated/infrastructure_recommendations.json')); print(d['summary']['need_based_projects'])" 2>/dev/null || echo "N/A")
            AI_IMPACT=$(python3 -c "import json; d=json.load(open('backend/data/simulated/infrastructure_recommendations.json')); print(f\"{d['summary']['ai_disparate_impact']:.1%}\")" 2>/dev/null || echo "N/A")
            NEED_IMPACT=$(python3 -c "import json; d=json.load(open('backend/data/simulated/infrastructure_recommendations.json')); print(f\"{d['summary']['need_based_disparate_impact']:.1%}\")" 2>/dev/null || echo "N/A")

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Test 3: Infrastructure Recommendations" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | AI Allocation | Need-Based |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|---------------|------------|" >> $GITHUB_STEP_SUMMARY
            echo "| Projects | $AI_PROJECTS | $NEED_PROJECTS |" >> $GITHUB_STEP_SUMMARY
            echo "| Disparate Impact | $AI_IMPACT | $NEED_IMPACT |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f backend/data/simulated/crash_predictions.json ]; then
            TOTAL_CRASHES=$(python3 -c "import json; d=json.load(open('backend/data/simulated/crash_predictions.json')); print(d['summary']['total_actual_crashes'])" 2>/dev/null || echo "N/A")
            Q1_BIAS=$(python3 -c "import json; d=json.load(open('backend/data/simulated/crash_predictions.json')); print(f\"{d['bias_by_quintile']['Q1 (Poorest)']['prediction_bias_pct']:.1f}%\")" 2>/dev/null || echo "N/A")
            Q5_BIAS=$(python3 -c "import json; d=json.load(open('backend/data/simulated/crash_predictions.json')); print(f\"{d['bias_by_quintile']['Q5 (Richest)']['prediction_bias_pct']:.1f}%\")" 2>/dev/null || echo "N/A")
            REPORTING_RATE=$(python3 -c "import json; d=json.load(open('backend/data/simulated/crash_predictions.json')); print(f\"{d['summary']['overall_reporting_rate']:.1%}\")" 2>/dev/null || echo "N/A")

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Test 2: Crash Prediction Bias" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Crashes (5yr) | $TOTAL_CRASHES |" >> $GITHUB_STEP_SUMMARY
            echo "| Q1 Prediction Bias | $Q1_BIAS |" >> $GITHUB_STEP_SUMMARY
            echo "| Q5 Prediction Bias | $Q5_BIAS |" >> $GITHUB_STEP_SUMMARY
            echo "| Overall Reporting Rate | $REPORTING_RATE |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f backend/data/simulated/demand_analysis.json ]; then
            SUPPRESSED_DEMAND=$(python3 -c "import json; d=json.load(open('backend/data/simulated/demand_analysis.json')); print(f\"{d['summary']['total_suppressed_demand']:,}\")" 2>/dev/null || echo "N/A")
            SUPPRESSION_RATE=$(python3 -c "import json; d=json.load(open('backend/data/simulated/demand_analysis.json')); print(f\"{d['summary']['suppression_rate']:.1f}%\")" 2>/dev/null || echo "N/A")
            NAIVE_AI_CORR=$(python3 -c "import json; d=json.load(open('backend/data/simulated/demand_analysis.json')); print(f\"{d['summary']['naive_ai_correlation']:.3f}\")" 2>/dev/null || echo "N/A")
            SOPH_AI_CORR=$(python3 -c "import json; d=json.load(open('backend/data/simulated/demand_analysis.json')); print(f\"{d['summary']['sophisticated_ai_correlation']:.3f}\")" 2>/dev/null || echo "N/A")

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Test 4: Suppressed Demand Analysis" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Suppressed Demand | $SUPPRESSED_DEMAND trips/day |" >> $GITHUB_STEP_SUMMARY
            echo "| Suppression Rate | $SUPPRESSION_RATE |" >> $GITHUB_STEP_SUMMARY
            echo "| Naive AI Correlation | $NAIVE_AI_CORR |" >> $GITHUB_STEP_SUMMARY
            echo "| Sophisticated AI Correlation | $SOPH_AI_CORR |" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload simulation data artifact
        uses: actions/upload-artifact@v4
        with:
          name: simulation-data
          path: backend/data/simulated/
          retention-days: 7

  generate-static-files:
    needs: [fetch-census-data, simulate-ai-predictions]
    if: needs.fetch-census-data.outputs.should-skip != 'true'
    runs-on: ubuntu-latest
    env:
      DATA_HASH: ${{ needs.fetch-census-data.outputs.data-hash }}
      GITHUB_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
      REPO_URL: ${{ github.server_url }}/${{ github.repository }}
      GIT_SHA: ${{ github.sha }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Download census data
        uses: actions/download-artifact@v4
        with:
          name: census-data
          path: backend/data/raw

      - name: Download simulation data
        uses: actions/download-artifact@v4
        with:
          name: simulation-data
          path: backend/data/simulated

      - name: Generate static JSON files for gh-pages
        run: |
          mkdir -p frontend/public/data
          python scripts/generate_static_data.py

          # Add metadata
          cat > frontend/public/data/metadata.json << EOF
          {
            "generated_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "data_hash": "${{ env.DATA_HASH }}",
            "github_run_url": "${{ env.GITHUB_RUN_URL }}",
            "git_sha": "${{ env.GIT_SHA }}"
          }
          EOF

      - name: Generate job summary
        run: |
          echo "## ðŸ“¦ Static Files Generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| File | Size |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|" >> $GITHUB_STEP_SUMMARY

          cd frontend/public/data
          for file in *.json; do
            SIZE=$(du -h "$file" | cut -f1)
            echo "| $file | $SIZE |" >> $GITHUB_STEP_SUMMARY
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Verification" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Hash:** \`${{ env.DATA_HASH }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Generated:** $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** [\`${GITHUB_SHA:0:7}\`](${{ env.REPO_URL }}/commit/${{ env.GIT_SHA }})" >> $GITHUB_STEP_SUMMARY

      - name: Upload static files artifact
        uses: actions/upload-artifact@v4
        with:
          name: static-data-files
          path: frontend/public/data/
          retention-days: 30

      - name: Generate final summary
        run: |
          echo "## âœ… Pipeline Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Static data files generated and uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
          echo "Run the deploy workflow to update the live site." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifacts:** Available for 30 days" >> $GITHUB_STEP_SUMMARY
          echo "**Data Hash:** ${{ env.DATA_HASH }}" >> $GITHUB_STEP_SUMMARY

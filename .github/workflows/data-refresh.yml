name: Data Refresh

on:
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - 'scripts/**'
      - '.github/workflows/data-refresh.yml'
  schedule:
    - cron: '0 6 * * 1'
  workflow_dispatch:
    inputs:
      force_regenerate:
        description: 'Force regenerate all data'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  actions: write

env:
  PYTHON_VERSION: '3.9'

concurrency:
  group: data-refresh
  cancel-in-progress: false

jobs:
  refresh:
    runs-on: ubuntu-slim
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements-pipeline.txt

      - name: Install dependencies
        run: pip install -r backend/requirements-pipeline.txt

      - name: Fetch source data
        env:
          CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}
        run: |
          python scripts/fetch_durham_data.py &
          python scripts/fetch_ncdot_nonmotorist.py &
          wait
          python scripts/fetch_osm_infrastructure.py

      - name: Validate source data
        run: |
          python3 -c "
          import json, csv, sys
          sys.path.insert(0, 'backend')
          from config import PLAUSIBILITY_RANGES

          errors = []

          # Census tracts
          with open('backend/data/raw/durham_census_tracts.geojson') as f:
              tracts = json.load(f)
          features = tracts['features']
          count = len(features)
          lo, hi = PLAUSIBILITY_RANGES['census_tracts']
          if not (lo <= count <= hi):
              errors.append(f'Census tract count {count} outside expected range {lo}-{hi}')

          for feat in features:
              props = feat['properties']
              income = props.get('median_household_income')
              if income is not None and income < 0:
                  errors.append(f'Negative income {income} in tract {props.get(\"GEOID\", \"?\")}')
              pop = props.get('total_population', 0)
              if pop == 0:
                  errors.append(f'Zero population in tract {props.get(\"GEOID\", \"?\")}')

          # Non-motorist crashes (real geocoded data)
          nm_path = 'backend/data/raw/ncdot_nonmotorist_durham.csv'
          with open(nm_path) as f:
              nm_reader = csv.DictReader(f)
              nm_rows = list(nm_reader)
          if len(nm_rows) < 100:
              errors.append(f'Non-motorist CSV has only {len(nm_rows)} records, expected >100')
          nm_missing_coords = sum(1 for r in nm_rows if not r.get('Latitude') or not r.get('Longitude'))
          if nm_missing_coords > 0:
              errors.append(f'Non-motorist CSV has {nm_missing_coords} rows with missing coordinates')

          # OSM infrastructure
          with open('backend/data/raw/osm_infrastructure.json') as f:
              osm = json.load(f)
          osm_totals = osm['totals']
          for key in ['crossings', 'bike_infra', 'traffic_signals', 'footways']:
              range_key = f'osm_{key}_total'
              if range_key in PLAUSIBILITY_RANGES:
                  lo, hi = PLAUSIBILITY_RANGES[range_key]
                  val = osm_totals.get(key, 0)
                  if not (lo <= val <= hi):
                      errors.append(f'OSM {key} count {val} outside expected range {lo}-{hi}')

          if errors:
              print('Source data validation failed:')
              for e in errors:
                  print(f'  - {e}')
              sys.exit(1)
          osm_total = sum(osm_totals.values())
          print(f'Source data valid: {count} tracts, {len(nm_rows)} non-motorist crashes, {osm_total} OSM features')
          "

      - name: Compute data hash
        id: compute-hash
        run: |
          HASH=$(cat \
            backend/data/raw/durham_census_tracts.geojson \
            backend/data/raw/ncdot_nonmotorist_durham.csv \
            backend/data/raw/osm_infrastructure.json \
            | sha256sum | cut -d' ' -f1 | cut -c1-16)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Restore previous data hash
        if: github.event.inputs.force_regenerate != 'true'
        id: cache-hash
        uses: actions/cache/restore@v4
        with:
          path: .data-hash
          key: durham-data-hash-latest
          restore-keys: durham-data-hash-

      - name: Check if data changed
        id: check-changed
        run: |
          CURRENT="${{ steps.compute-hash.outputs.hash }}"
          FORCE="${{ github.event.inputs.force_regenerate }}"

          git fetch origin main --depth=1 2>/dev/null || true
          CODE_CHANGED=$(git diff --quiet HEAD origin/main -- backend/ scripts/ 2>/dev/null && echo "false" || echo "true")

          DATA_CHANGED="true"
          if [ -f .data-hash ]; then
            PREV_HASH=$(cat .data-hash)
            if [ "$CURRENT" == "$PREV_HASH" ]; then
              DATA_CHANGED="false"
            fi
          fi

          if [[ "${{ github.event_name }}" == "schedule" && "$DATA_CHANGED" == "false" && "$CODE_CHANGED" == "false" && "$FORCE" != "true" ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "Skipping: data unchanged, no code changes"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Save data hash
        if: steps.check-changed.outputs.skip != 'true'
        uses: actions/cache/save@v4
        with:
          path: .data-hash
          key: durham-data-hash-latest

      - name: Run simulations
        if: steps.check-changed.outputs.skip != 'true'
        run: |
          python scripts/simulate_ai_predictions.py &
          python scripts/simulate_crash_predictions.py &
          python scripts/simulate_infrastructure_recommendations.py &
          python scripts/analyze_suppressed_demand.py &
          wait

      - name: Generate static JSON files
        if: steps.check-changed.outputs.skip != 'true'
        run: |
          mkdir -p frontend/public/data
          python scripts/generate_static_data.py

      - name: Validate generated data
        if: steps.check-changed.outputs.skip != 'true'
        run: |
          python3 -c "
          import json, os, sys
          sys.path.insert(0, 'backend')
          from config import PLAUSIBILITY_RANGES

          errors = []
          data_dir = 'frontend/public/data'

          expected_files = [
              'census-tracts.json',
              'counter-locations.json',
              'volume-report.json',
              'choropleth-data.json',
              'accuracy-by-income.json',
              'accuracy-by-race.json',
              'scatter-data.json',
              'crash-report.json',
              'confusion-matrices.json',
              'crash-time-series.json',
              'crash-geo-data.json',
              'infrastructure-report.json',
              'danger-scores.json',
              'budget-allocation.json',
              'recommendations.json',
              'demand-report.json',
              'demand-funnel.json',
              'detection-scorecard.json',
              'demand-geo-data.json',
              'metadata.json',
              'data-manifest.json',
          ]

          # Check all files exist and parse as JSON
          for fname in expected_files:
              path = os.path.join(data_dir, fname)
              if not os.path.exists(path):
                  errors.append(f'Missing file: {fname}')
                  continue
              try:
                  with open(path) as f:
                      json.load(f)
              except json.JSONDecodeError as e:
                  errors.append(f'Invalid JSON in {fname}: {e}')

          # Check crash-geo-data.json uses actual_crashes (not stale actual_crashes_5yr)
          crash_path = os.path.join(data_dir, 'crash-geo-data.json')
          if os.path.exists(crash_path):
              with open(crash_path) as f:
                  crash_data = json.load(f)
              props = crash_data['features'][0]['properties']
              if 'actual_crashes' not in props:
                  errors.append('crash-geo-data.json missing actual_crashes field')
              if 'actual_crashes_5yr' in props:
                  errors.append('crash-geo-data.json contains stale actual_crashes_5yr field')

          # Check budget by_quintile sums to expected range
          budget_lo, budget_hi = PLAUSIBILITY_RANGES['budget_allocation_total']
          report_path = os.path.join(data_dir, 'infrastructure-report.json')
          if os.path.exists(report_path):
              with open(report_path) as f:
                  report = json.load(f)
              for alloc_key in ['ai_allocation', 'need_based_allocation']:
                  by_q = report['equity_metrics'][alloc_key]['by_quintile']
                  total = sum(by_q.values())
                  if not (budget_lo <= total <= budget_hi):
                      errors.append(f'{alloc_key} by_quintile sum {total} outside {budget_lo/1e6:.1f}M-{budget_hi/1e6:.1f}M range')

          # Check danger-scores feature count matches census-tracts
          tracts_path = os.path.join(data_dir, 'census-tracts.json')
          danger_path = os.path.join(data_dir, 'danger-scores.json')
          if os.path.exists(tracts_path) and os.path.exists(danger_path):
              with open(tracts_path) as f:
                  tract_count = len(json.load(f)['features'])
              with open(danger_path) as f:
                  danger_count = len(json.load(f)['features'])
              if tract_count != danger_count:
                  errors.append(f'Feature count mismatch: census-tracts has {tract_count}, danger-scores has {danger_count}')

          # Check confusion matrices have meaningful variation (not trivially perfect)
          cm_path = os.path.join(data_dir, 'confusion-matrices.json')
          if os.path.exists(cm_path):
              with open(cm_path) as f:
                  cm_data = json.load(f)
              f1_scores = [v['f1_score'] for v in cm_data['by_quintile'].values()]
              if len(f1_scores) >= 2:
                  f1_spread = max(f1_scores) - min(f1_scores)
                  min_spread = PLAUSIBILITY_RANGES['confusion_matrix_min_f1_spread']
                  if f1_spread < min_spread:
                      errors.append(
                          f'Confusion matrix F1 spread {f1_spread:.3f} below {min_spread} — '
                          f'classification may be trivially easy (all F1: {[round(f, 2) for f in f1_scores]})'
                      )

          if errors:
              print('Generated data validation failed:')
              for e in errors:
                  print(f'  - {e}')
              sys.exit(1)
          print(f'Generated data valid: {len(expected_files)} files, {tract_count} tracts, feature counts match')
          "

      - name: Validate data plausibility
        if: steps.check-changed.outputs.skip != 'true'
        run: |
          python3 -c "
          import json, sys
          sys.path.insert(0, 'backend')
          from config import PLAUSIBILITY_RANGES

          errors = []
          data_dir = 'frontend/public/data'

          # Crash volumes
          with open(f'{data_dir}/crash-report.json') as f:
              crash = json.load(f)
          total = crash['summary']['total_crashes_all_years']
          years = len(crash['summary']['years_analyzed'])
          per_year = total / years

          lo, hi = PLAUSIBILITY_RANGES['crashes_per_year']
          if not (lo <= per_year <= hi):
              errors.append(f'Crashes per year {per_year:.0f} outside range {lo}-{hi}')

          lo, hi = PLAUSIBILITY_RANGES['crashes_total_5yr']
          if not (lo <= total <= hi):
              errors.append(f'Total crashes {total} outside range {lo}-{hi}')

          # Population and income
          with open(f'{data_dir}/census-tracts.json') as f:
              tracts = json.load(f)

          total_pop = sum(
              f['properties'].get('total_population', 0)
              for f in tracts['features']
          )
          lo, hi = PLAUSIBILITY_RANGES['durham_total_population']
          if not (lo <= total_pop <= hi):
              errors.append(f'Total population {total_pop} outside range {lo}-{hi}')

          lo, hi = PLAUSIBILITY_RANGES['census_tracts']
          tract_count = len(tracts['features'])
          if not (lo <= tract_count <= hi):
              errors.append(f'Census tract count {tract_count} outside range {lo}-{hi}')

          inc_lo, inc_hi = PLAUSIBILITY_RANGES['median_income_range']
          for feat in tracts['features']:
              income = feat['properties'].get('median_income')
              if income is not None and not (inc_lo <= income <= inc_hi):
                  tid = feat['properties'].get('tract_id', '?')
                  errors.append(f'Tract {tid} income {income} outside range {inc_lo}-{inc_hi}')

          if errors:
              print('Plausibility validation failed:')
              for e in errors:
                  print(f'  - {e}')
              sys.exit(1)
          print(f'Plausibility valid: {per_year:.0f} crashes/yr, {total_pop:,} population, {tract_count} tracts')
          "

      - name: Push to data branch
        if: steps.check-changed.outputs.skip != 'true'
        run: |
          git clone --branch data --single-branch --depth 1 \
            "https://x-access-token:${{ github.token }}@github.com/${{ github.repository }}.git" \
            /tmp/data-branch

          cd /tmp/data-branch
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          rm -rf raw/ simulated/ frontend/
          cp -r "$GITHUB_WORKSPACE/backend/data/raw" raw/
          cp -r "$GITHUB_WORKSPACE/backend/data/simulated" simulated/
          cp -r "$GITHUB_WORKSPACE/frontend/public/data" frontend/

          find . -name '.gitkeep' -delete 2>/dev/null || true

          git add -A
          if git diff --cached --quiet; then
            echo "No data changes to commit"
          else
            SUMMARY=$(python3 -c "
          import json, csv, os
          tracts = json.load(open('raw/durham_census_tracts.geojson'))
          with open('raw/ncdot_nonmotorist_durham.csv') as f:
              crash_count = sum(1 for _ in csv.reader(f)) - 1
          frontend_files = [f for f in os.listdir('frontend') if f.endswith('.json')]
          print(f'{len(tracts[\"features\"])} tracts, {crash_count} non-motorist crashes, {len(frontend_files)} frontend files')
          ")
            git commit -m "data: refresh $(date -u +'%Y-%m-%d') — $SUMMARY"
            git push origin data
          fi

      - name: Trigger deploy
        if: steps.check-changed.outputs.skip != 'true'
        run: gh workflow run deploy.yml --ref main
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Generate job summary
        if: always()
        run: |
          echo "## Data Refresh" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.check-changed.outputs.skip }}" == "true" ]; then
            echo "> Skipped: data unchanged, no code changes" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          echo "### Source Data" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY

          if [ -f backend/data/raw/durham_census_tracts.geojson ]; then
            TRACTS=$(python3 -c "import json; d=json.load(open('backend/data/raw/durham_census_tracts.geojson')); print(len(d['features']))")
            CENSUS_SIZE=$(du -h backend/data/raw/durham_census_tracts.geojson | cut -f1)
            echo "| Census Tracts | $TRACTS |" >> $GITHUB_STEP_SUMMARY
            echo "| Census File | $CENSUS_SIZE |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f backend/data/raw/osm_infrastructure.json ]; then
            OSM_FEATURES=$(python3 -c "import json; d=json.load(open('backend/data/raw/osm_infrastructure.json')); print(sum(d['totals'].values()))")
            OSM_SIZE=$(du -h backend/data/raw/osm_infrastructure.json | cut -f1)
            echo "| OSM Features | $OSM_FEATURES |" >> $GITHUB_STEP_SUMMARY
            echo "| OSM File | $OSM_SIZE |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f backend/data/raw/ncdot_nonmotorist_durham.csv ]; then
            NM_COUNT=$(python3 -c "import csv; print(sum(1 for _ in csv.reader(open('backend/data/raw/ncdot_nonmotorist_durham.csv'))) - 1)")
            NM_SIZE=$(du -h backend/data/raw/ncdot_nonmotorist_durham.csv | cut -f1)
            echo "| Non-Motorist Crashes | $NM_COUNT |" >> $GITHUB_STEP_SUMMARY
            echo "| Non-Motorist File | $NM_SIZE |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -d "frontend/public/data" ]; then
            echo "### Generated Files" >> $GITHUB_STEP_SUMMARY
            echo "| File | Size |" >> $GITHUB_STEP_SUMMARY
            echo "|------|------|" >> $GITHUB_STEP_SUMMARY
            cd frontend/public/data
            for file in *.json; do
              SIZE=$(du -h "$file" | cut -f1)
              echo "| $file | $SIZE |" >> $GITHUB_STEP_SUMMARY
            done
            cd -
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| | |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Data Hash | \`${{ steps.compute-hash.outputs.hash }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Timestamp | $(date -u +"%Y-%m-%d %H:%M:%S UTC") |" >> $GITHUB_STEP_SUMMARY

# Data Pipeline

Automated data pipeline that runs weekly to update Durham transportation safety analysis.

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Weekly Cron    â”‚  Every Monday 6 AM UTC
â”‚  or Manual      â”‚  or Push to backend/scripts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 1: Fetch Census Data                                   â”‚
â”‚  - Fetch Durham census tracts (238 tracts)                 â”‚
â”‚  - Calculate data hash                                      â”‚
â”‚  - Skip if unchanged (scheduled runs)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 2: Simulate AI Predictions                             â”‚
â”‚  - Generate 15 counter locations                            â”‚
â”‚  - Apply documented bias patterns                           â”‚
â”‚    â€¢ Low-income: -25% accuracy                             â”‚
â”‚    â€¢ High-income: +8% accuracy                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 3: Generate Static Files                               â”‚
â”‚  - Run backend analysis (Flask + GeoPandas)                â”‚
â”‚  - Export 8 JSON files to frontend/public/data/           â”‚
â”‚  - Add metadata (hash, timestamp, commit)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 4: Commit & Deploy                                     â”‚
â”‚  - Commit updated JSON files                                â”‚
â”‚  - Push to main branch                                      â”‚
â”‚  - Trigger gh-pages deployment automatically               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
   Live at: civic-ai-audits.github.io/durham-transport
```

## ğŸš€ Quick Start

### Run Pipeline Manually

```bash
# Standard run (skip if data unchanged)
gh workflow run data-pipeline.yml --repo civic-ai-audits/durham-transport

# Force regenerate (even if data unchanged)
gh workflow run data-pipeline.yml \
  --repo civic-ai-audits/durham-transport \
  -f force_regenerate=true
```

Or via web: https://github.com/civic-ai-audits/durham-transport/actions/workflows/data-pipeline.yml

### Check Pipeline Status

```bash
gh run list --repo civic-ai-audits/durham-transport --workflow=data-pipeline.yml --limit 3
```

### View Pipeline Logs

```bash
gh run view <RUN_ID> --repo civic-ai-audits/durham-transport --log
```

---

## ğŸ“‹ Pipeline Details

### Triggers

1. **Scheduled:** Weekly on Mondays at 6 AM UTC
   - Census data updates quarterly, weekly checks are sufficient
   - Skips if data hasn't changed (saves GitHub Actions minutes)

2. **Push to Main:** Changes to backend or scripts
   - Auto-runs when analysis code changes
   - Regenerates data with new logic

3. **Manual:** workflow_dispatch
   - Run anytime via CLI or web
   - Option to force regenerate

### Smart Caching

Pipeline uses data hashing (like duke-mlk):

```bash
# Calculate hash of census data
HASH=$(sha256sum durham_census_tracts.geojson | cut -c1-16)

# Compare with previous run
if [[ scheduled && HASH == PREV_HASH ]]; then
  echo "Data unchanged, skipping pipeline"
  # Saves ~5-10 minutes of GitHub Actions time
fi
```

**Benefits:**
- Saves GitHub Actions minutes (2,000 free/month)
- Only regenerates when data actually changes
- Can force regenerate anytime if needed

---

## ğŸ“Š Generated Files

### Static JSON Files (frontend/public/data/)

| File | Description | Size |
|------|-------------|------|
| `census-tracts.json` | 238 Durham census tracts with geometries | ~313 KB |
| `choropleth-data.json` | Tract-level error data for map | ~332 KB |
| `report.json` | Complete audit report | ~17 KB |
| `accuracy-by-income.json` | Income quintile analysis | ~1.5 KB |
| `accuracy-by-race.json` | Racial composition analysis | ~1.1 KB |
| `scatter-data.json` | Predicted vs actual data points | ~11 KB |
| `error-distribution.json` | Error histogram data | ~2 KB |
| `counter-locations.json` | 15 counter locations | ~3.3 KB |
| `metadata.json` | Generation metadata | ~300 B |

**Total:** ~680 KB (uncompressed), ~180 KB (gzipped)

### Metadata Tracking

Each generation includes verification metadata:

```json
{
  "generated_at": "2026-02-07T16:00:00Z",
  "data_hash": "a1b2c3d4e5f6g7h8",
  "github_run_url": "https://github.com/.../actions/runs/123456",
  "git_sha": "abc123..."
}
```

This enables:
- Reproducibility (know exactly which code/data version)
- Verification (check data integrity with hash)
- Audit trail (track when and why data changed)

---

## ğŸ”§ Pipeline Jobs

### Job 1: Fetch Census Data

**Purpose:** Get latest Durham census demographics

**Actions:**
- Tries US Census Bureau API (if `CENSUS_API_KEY` secret set)
- Falls back to synthetic data generation
- Calculates hash to detect changes
- Uploads as artifact for next jobs

**Runtime:** ~1-2 minutes

**Skips if:**
- Scheduled run + data hash unchanged

### Job 2: Simulate AI Predictions

**Purpose:** Generate AI volume predictions with bias

**Actions:**
- Creates 15 counter locations across income spectrum
- Generates ground truth volumes
- Applies documented bias patterns
- Calibrated to research literature

**Runtime:** ~30 seconds

### Job 3: Generate Static Files

**Purpose:** Run full backend analysis and export results

**Actions:**
- Loads census + simulation data
- Runs VolumeEstimationAuditor
- Calculates all equity metrics
- Exports 8 JSON files for gh-pages
- Adds verification metadata

**Runtime:** ~1-2 minutes

### Job 4: Commit & Deploy

**Purpose:** Update repository and trigger deployment

**Actions:**
- Commits new JSON files to main branch
- Git bot commits with pipeline metadata
- Push triggers gh-pages deploy workflow
- Site updates automatically

**Runtime:** ~30 seconds

**Total Pipeline:** ~5-7 minutes end-to-end

---

## ğŸ“ˆ Comparison: Pipeline vs Long-Running Server

### Pipeline Approach (Current) âœ…

```
Weekly Schedule â†’ Generate Data â†’ Commit â†’ Deploy
     6 min          5 min          1 min    2 min

Total: ~15 minutes/week = 1 hour/month
Cost: FREE (well under 2,000 minutes)
Availability: 24/7 via gh-pages
```

**Pros:**
- âœ… Always available (gh-pages CDN)
- âœ… No runtime costs between updates
- âœ… Fast (CDN-cached static files)
- âœ… Minimal Actions minutes usage
- âœ… Automatic updates when data changes

**Cons:**
- âš ï¸ Data refreshes weekly (not real-time)
- âš ï¸ Can't do custom user queries

### Long-Running Server (Alternative) âŒ

```
GitHub Action â†’ Start Flask â†’ Keep Alive â†’ Shutdown
                             6 hours

Total: 360 minutes per run
Cost: 1 run = 18% of monthly free tier
Availability: Only while running
```

**Pros:**
- âœ… Real-time analysis
- âœ… Can handle custom queries

**Cons:**
- âŒ Limited availability (6-12 hours max)
- âŒ Burns GitHub Actions minutes fast
- âŒ Requires ngrok for public access
- âŒ URL changes each restart
- âŒ Not suitable for production

---

## ğŸ¯ When to Use What

### Use Pipeline (Default)

**Perfect for:**
- âœ… Static datasets that change infrequently (census data)
- âœ… Fixed analysis (pre-defined visualizations)
- âœ… Public demos and sharing
- âœ… Production deployments
- âœ… Cost-conscious projects

**Your use case:** Durham census changes quarterly, analysis is fixed
â†’ **Pipeline is ideal**

### Use Long-Running Server

**Only if you need:**
- User uploads their own city data
- Custom filters/date ranges
- Real-time "what-if" scenarios
- Dynamic analysis based on user input

**Not your use case** (fixed Durham data, fixed visualizations)

---

## ğŸ” Monitoring

### View Last Pipeline Run

```bash
gh run list --repo civic-ai-audits/durham-transport \
  --workflow=data-pipeline.yml \
  --limit 1
```

### Check if Data is Stale

```bash
# Get last update time from metadata
curl -s https://civic-ai-audits.github.io/durham-transport/data/metadata.json | \
  jq -r '.generated_at'
```

### Force Fresh Data

```bash
gh workflow run data-pipeline.yml \
  --repo civic-ai-audits/durham-transport \
  -f force_regenerate=true
```

---

## ğŸ” Secrets (Optional)

### CENSUS_API_KEY

Get free API key: https://api.census.gov/data/key_signup.html

Add to repository secrets:
1. Go to: https://github.com/civic-ai-audits/durham-transport/settings/secrets/actions
2. Click "New repository secret"
3. Name: `CENSUS_API_KEY`
4. Value: [your key]

**Benefits:**
- Access live Census API instead of synthetic data
- Higher rate limits
- More reliable data fetching

**Not required:** Pipeline works fine without it (uses synthetic data)

---

## ğŸ“Š Cost Analysis

### GitHub Actions Free Tier

- **Monthly limit:** 2,000 minutes
- **Pipeline runtime:** ~6 minutes/run
- **Weekly schedule:** 4 runs/month = 24 minutes
- **Manual runs:** ~5 runs/month = 30 minutes
- **Push triggers:** ~10 runs/month = 60 minutes

**Total: ~114 minutes/month (6% of free tier)** âœ…

Compare to long-running server:
- 1 demo = 360 minutes (18% of tier)
- 3 demos/month = 1,080 minutes (54% of tier)
- 6 demos/month = 2,160 minutes (over limit, costs money)

---

## ğŸ“ Inspired By

This pipeline is modeled after [`duke-mlk/medical-flow`](../../duke-mlk/medical-flow):
- Smart caching with data hashing
- Multi-job workflow with artifacts
- Automated report generation
- Metadata tracking for reproducibility

Key pattern borrowed:
```yaml
# Only run expensive jobs if data changed
if: needs.fetch-data.outputs.should-skip != 'true'
```

---

## ğŸ“š Additional Resources

- **GitHub Actions docs:** https://docs.github.com/en/actions
- **Census API:** https://www.census.gov/data/developers/data-sets.html
- **Workflow file:** `.github/workflows/data-pipeline.yml`
- **Live site:** https://civic-ai-audits.github.io/durham-transport/
